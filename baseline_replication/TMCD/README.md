# Compositional Generalization and Natural Language Variation: Can a Semantic Parsing Approach Handle Both?

This directory contains code related to the paper "Compositional Generalization
and Natural Language Variation: Can a Semantic Parsing Approach Handle Both?"
(Peter Shaw, Ming-Wei Chang, Panupong Pasupat, Kristina Toutanova).

https://aclanthology.org/2021.acl-long.75/

### Setup and Prerequisites

All python scripts should be run using Python 3 while in the top-level of this
repository using `-m`. For example:

```shell
python -m language.compgen.nqg.tasks.geoquery.write_dataset
```

Widely used prerequisite modules are `absl-py` and `tensorflow`. Additionally, for Spider preprocessing, `sqlparse` is used. For NQG parsing models, the `tf-models-official` library is required:

https://github.com/tensorflow/models/tree/master/official#how-to-get-started-with-the-official-models

## Datasets

Below are instructions for reproducing the dataset splits used in the paper.

We use a standard TSV format for representing all splits, where each
line corresponds to an example and is formatted as:

`<source>\t<target>\n`

Where `<source>` is the input string and `<target>` is the output string.

### SCAN

The "add primitive" and "length" splits, as well as the original dataset,
are available here:

https://github.com/brendenlake/SCAN

Instructions to produce the SCAN MCD splits are here:

https://github.com/google-research/google-research/tree/master/cfq#scan-mcd-splits

The SCAN files can be converted to our TSV format by using the
`tasks/scan/convert_to_tsv.py` script for files in the original dataset format,
and `tasks/scan/join_txt_to_tsv.py` to join the input and output txt files
generated for the MCD splits.

### GeoQuery

You can learn more about the GeoQuery dataset here:
https://www.cs.utexas.edu/users/ml/nldata/geoquery.html

You can download the GeoQuery corpus with FunQL annotated expressions here:
http://www.cs.utexas.edu/~ml/wasp/geo-funql/corpus.xml

You will also need to download the geobase file which is used to identify
entities to replace with placeholders:
[ftp://ftp.cs.utexas.edu/pub/mooney/nl-ilp-data/geosystem/geobase](ftp://ftp.cs.utexas.edu/pub/mooney/nl-ilp-data/geosystem/geobase)

You can then generate the dataset in TSV format using the
`tasks/geoquery/write_dataset.py` script.

Example usage:

```shell
python -m language.compgen.nqg.tasks.geoquery.write_dataset \
--corpus=/path/to/.../geoquery.xml \
--geobase=/path/to/.../geobase \
--output=/path/to/.../dataset.tsv
```

You can then reproduce the four splits used in the paper using the
`tasks/split_dataset.py` script, with the output of `write_dataset.py`
as `--input` and a file in `tasks/geoquery/splits` as `--split`.

### Spider

The Spider dataset can be downloaded from this location:
https://yale-lily.github.io/spider

Below, we will assume that the environment variable `SPIDER_DIR` points to a
directory that contains the Spider dataset.

In the paper we use various splits of a dataset we refer to as Spider-SSP which
contains all examples in the original Spider training set for databases with
at least 50 examples. This set of databases can be determined using the
`tasks/spider/print_database_counts.py` script, but is also hardcoded in
`tasks/spider/database_constants.py`.

You can generate the Spider-SSP dataset in TSV format using the
`tasks/spider/write_dataset.py` script with
`--examples=${SPIDER_DIR}/data/train_spider.json`. For Spider-XSP, you should
set `--filter_by_database=False`.

When using T5, we append a serialized database schema to the input string.
This can be accomplished using the `tasks/spider/append_schema.py` script with
`--input` set to the TSV generated by `tasks/spider/write_dataset.py` and
`--tables=${SPIDER_DIR}/data/tables.json`.

You can then reproduce the four splits used in the paper using the
`tasks/split_dataset.py` script, with the output of `write_dataset.py` or
`append_schema.py` as `--input` and a file in `tasks/spider/splits` as
`--split`.

For evaluation, you will need to download the Spider `evaluation.py` script
from: https://github.com/taoyds/spider.

You can then use `tasks/spider/generate_gold.py` to generate an input file
for the `--gold` flag of `evaluation.py`. The `--preds` flag should point to a
txt file of generated predictions.

Note that for T5, you will need to run `tasks/spider/restore_oov.py` to
post-process generated predictions.

### Generating New Dataset Splits

For generating new random and length splits, the `tasks/gen_length_split.py`
and `tasks/gen_random_split.py` tools can be applied to any dataset in the TSV
format described above. We describe how to generate new template and TMCD
splits below.

#### Template Splits

For generating new template splits for GeoQuery using the same template
definition used in the paper, you can use the
`tasks/geoquery/gen_template_split.py` tool, with the output of
`tasks/geoquery/write_dataset.py` as `--input`.

Similarly, for Spider, you can use the `tasks/spider/gen_template_split.py`
tool, with the output of `tasks/spider/write_dataset.py` or
`tasks/spider/append_schema.py` as `--input`.

For generating template splits for new datasets, or for generating template
splits for the datasets studied in this work using a different procedure to
determine target templates, you can use the utilities in
`tasks/template_utils.py`.

#### TMCD Splits

To apply the TMCD methodology to new datasets, or to change the definition
of atoms and compounds for the dataset studied in this paper, you may find
the utilities in `tasks/mcd_utils.py` useful. The functions in this module
require you to define functions to map examples to atoms and compounds,
but you can then generate new TMCD splits and compute various metrics.

You can find several scripts related to TMCD and the corresponding notions
of compound divergence in both the `tasks/geoquery` and
`tasks/spider` sub-directories:

* `gen_tmcd_split.py` - Tool to generate a new TMCD split, given a dataset in TSV format.
* `measure_compound_divergence.py` - Tool to measure compound divergence for any split.
* `measure_unseen_atoms.py` - Tool to measure the number of examples containing unseen atoms for any split.

Note that there may also be interest in applying a different atom constraint,
e.g. a constraint based on atom divergence (similarly to Keysers et al. 2020)
as opposed to the constraint used in the paper, depending on the focus of
a given evaluation and the size of the datasets being considered.
Unfortunately, the code does not currently support
this functionality, but ideally the utilities in `tasks/mcd_utils.py` can
provide a helpful starting point.

Also note that definition of compounds for Spider released in this library uses
a slightly simplified and significantly more readable CFG for parsing SQL than
the grammar used to define compounds for the original paper, which we therefore
believe to be more useful for future work. Please contact the authors if you
have a specific need for the original compound definition.

## Approaches

### T5

Instructions for fine-tuning T5 given a dataset in the TSV format described
above are here:

https://github.com/google-research/text-to-text-transfer-transformer#using-a-tsv-file-directly

This document also contains instructions for generating predictions:

https://github.com/google-research/text-to-text-transfer-transformer#decode

To create a txt file with inputs to generate test predictions from a test tsv
file, you can use the `tasks/strip_targets.py` script. For datasets using
simple exact match (GeoQuery and SCAN), you can then compare these predictions
with the targets provided by a TSV file for a given test split using the script
`tasks/compare_predictions.py`.

#### T5 Hyperparameters

The default hyperparameters were used for T5, with the exception of learning
rate, which was set to `0.0001` for all experiments. Here are some of the
relevant hyperparameter used:

```
constant_learning_rate.learning_rate = 0.0001
tokens_per_batch = 1048576
AdafactorOptimizer.beta1 = 0.0
AdafactorOptimizer.clipping_threshold = 1.0
AdafactorOptimizer.decay_rate = None
AdafactorOptimizer.epsilon1 = 1e-30
AdafactorOptimizer.epsilon2 = 0.001
```

For training, we used maximum sequence lengths up to:

```
run.sequence_length = {'inputs': 512, 'targets': 256}
```

For inference, we used `sample_decode` with `temperature=0` and a
`max_decode_length` up to 256.

We used the prefix `semanticparse: ` for all T5 experiments.

Note: the appendix of the preprint states the batch size used for T5 was
128, which we recently discovered is incorrect (see `tokens_per_batch` above).
We are working to release a new version of the paper with this corrected.

### NQG

The `model/` directory contains code for a discriminative neural parsing model and a
Quasi-synchronous Context-Free Grammar (QCFG) induction algorithm, collectively
referred to as NQG.
The inference code also supports evaluations of NQG-T5, a combination of NQG
with T5, and other hybrids combining NQG with another higher coverage model.

The sub-directory structure of `model/` is:

- `qcfg` consists of common modules for representing and parsing QCFGs.
- `induction` consists of all of the code necessary for inducing QCFGs given a
set of examples.
- `parser` consists of code for training the BERT-based neural parsing model.

Note that the grammar induction algorithm can be used independently from the
neural parsing model, and conversely the neural parsing model can also be used
with any other method for producing a QCFG (such as manual annotation).

Input examples are expected to be in the `source\ttarget\n` TSV format used
elsewhere in this codebase. The following will expect that the environment
variables `TRAIN_TSV` and `TEST_TSV` point the train and test splits,
respectively.

#### QCFG Induction

Grammar induction requires only a dataset in TSV format. Various
options and hyperparameters are configured via command line flags.

Example usage:

```shell
python -m language.compgen.nqg.model.induction.induce_rules \
  --input=${TRAIN_TSV} \
  --output=${RULES} \
  --sample_size=500 \
  --terminal_codelength=32 \
  --allow_repeated_target_nts=true
```

As the computational complexity of grammar induction increases with the
length (in tokens) of the training examples and the total number of examples,
it is necessary to set `sample_size` appropriately so that grammar induction
can complete in a reasonable amount of time for your dataset. For reference,
we used the following hyperparameters.

* SCAN: `--sample_size=500 --terminal_codelength=32 --allow_repeated_target_nts=true`
* GeoQuery: `--sample_size=0 --terminal_codelength=8 --allow_repeated_target_nts=false`
* Spider: `--sample_size=1000 --terminal_codelength=8 --allow_repeated_target_nts=true`

Note that for Spider, the script `tasks/spider/nqg_preprocess.py` should be run
on the dataset TSV file to prepare the input for the space separated tokenization
used by NQG.

#### Neural Parsing Model Training

The neural model code is implemented in TF 2.x, using the BERT implementation from:

https://github.com/tensorflow/models/tree/master/official/nlp/bert#pre-trained-models

The model itself, defined in `nqg_model.py`, consists only of relatively
simple layers on top a BERT encoder. However, there is more complexity for model
training, which requires generating parse forest representations for the numerator of
the MML objective (which sums over derivations of the goal target) and the
denominator (a partition function which sums over all derivations of the input).
These parse forests are serialized into a set of integer tensors when writing the model training data. During training,
the training loop uses `tf.TensorArray` to sequentially iterate over the
serialized forests leveraging dynamic programming for efficiency. At inference time, model scores
are integrated with a
QCFG parsing algorithm to efficiently produce the highest scoring derivation,
and output the corresponding target.

JSON config files are used to configure parsing model hyperparameters.
Reference configs for SCAN, GeoQuery, and Spider are located in
the `model/parser/configs/` directory. The example commands below expect `CONFIG` to
point to such a JSON config file.

First, you need to generate training data, which is formatted as a TFRecord file
of `tf.Example` protobuffers. This is done using the `write_examples.py` script,
e.g.:

```shell
python -m language.compgen.nqg.model.parser.data.write_examples \
  --input=${TRAIN_TSV} \
  --output=${TF_EXAMPLES} \
  --config=${CONFIG} \
  --rules=${RULES} \
  --bert_dir=${BERT_DIR}
```

This will write training examples to `TF_EXAMPLES`.

`BERT_DIR` should point to BERT directory with a vocab file, a BERT config file,
and a TF 2.x compatible BERT checkpoint. See the following link for details on TF 2.x compatible BERT checkpoints:

https://github.com/tensorflow/models/tree/master/official/nlp/bert#pre-trained-models

You can then run model training locally as follows:

```shell
python -m language.compgen.nqg.model.parser.training.train_model \
  --input=${TF_EXAMPLES} \
  --config=${CONFIG} \
  --model_dir=${MODEL_DIR} \
  --bert_dir=${BERT_DIR}
```

Model checkpoints will be saved to `MODEL_DIR`.

`train_model.py` also supports distributed GPU training using the `--use_gpu` flag.

#### Inference and Evaluation

There are two scripts that useful for generating predictions and evaluation, `eval_model.py` and `generate_predictions.py`.

The `eval_model.py` script can be
run in parallel with model training. If run with `--poll` and `--write`, it will wait for new checkpoints to be written to `MODEL_DIR`, compute evaluation metrics, and then write metrics to `MODEL_DIR`. These metrics
can then be visualized in TensorBoard:

https://www.tensorflow.org/tensorboard

Here is an example usage:

```shell
python -m language.compgen.nqg.model.parser.inference.eval_model \
  --input=${TEST_TSV} \
  --config=${CONFIG} \
  --model_dir=${MODEL_DIR} \
  --bert_dir=${BERT_DIR} \
  --rules=${RULES}
```

The script computes the following metrics are computed for NQG (which can
abstrain from making a prediction):

* `nqg_accuracy` - Number of correct NQG predictions / number of test examples
* `nqg_coverage` - Number of NQG predictions / number of test examples
* `nqg_precision` - Number of correct NQG predictions / number of NQG predictions

To compute performance for a setting such as NQG-T5, you can optionally provide predictions (e.g. from T5 or some other high coverage model)
to be used in the case that NQG does not produce an output. These predictions are provided in the form of a txt
file with one prediction per line, and specified as the argument for the flag `--fallback_predictions`.
The `eval_model.py` script will also produce metrics for this setting:

* `fallback_accuracy` - Number of correct fallback predictions / number of test examples
* `hybrid_accuracy` - Number of correct hybrid predictions / number of test examples

The hybrid prediction for a given example will be the NQG prediction if one is generated, and the fallback prediction (if one is provided) otherwise.

You can also optionally provide a CFG defining valid targets for a particular task with the
flag `--target_grammar`. NQG predictions that cannot be parsed by this CFG will
be discarded. A CFG for FunQL as used by GeoQuery is provided in
`parser/inference/targets/funql.txt`, and a CFG for Spider can be generated by
`parser/inference/targets/generate_spider_grammars.py`.

The script `generate_predictions.py` accepts similar command line flags as `eval_model.py`, except
rather than accepting a TSV file as input, the input should be a txt file of inputs
to generate predicted targets for.
An input txt file can be generated from a TSV file using the `tasks/strip_targets.py` script.
Instead of computing evaluation metrics, a txt file of predictions is generated.
This script is therefore useful for generating predictions when the gold targets are not given.
This script is also useful for datasets such as Spider where an evaluation metric other than
exact string comparison is used, as the generated predictions can be used as input to some other evaluation script.
